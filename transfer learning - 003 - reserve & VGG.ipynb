{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning   \n",
    "### Based on:   \n",
    "https://medium.com/towards-data-science/transfer-learning-using-keras-d804b2e04ef8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('python = {0}'.format(sys.version))\n",
    "print('numpy = {0}'.format(np.__version__))\n",
    "print('pandas = {0}'.format(pd.__version__))\n",
    "print('keras = {0}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data. Make train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readScan(scanName,var_name):\n",
    "    data=h5py.File(scanName,'r')\n",
    "    Xscans=data.get(var_name)\n",
    "    X=copy.copy(Xscans.value)\n",
    "    X1=np.rollaxis(X,0,start=3)\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_echoes=100\n",
    "skip_rows=3 # the first 50 description files have a different format \n",
    "\n",
    "X_rest=[]\n",
    "X_stress=[]\n",
    "X_reserve=[]\n",
    "y=[]\n",
    "\n",
    "for idx,i in enumerate(np.arange(n_echoes)+1):\n",
    "    scan_rest='../stress_rest_reserve_100/rest{0}.mat'.format(i)\n",
    "    X_rest.append(readScan(scan_rest,'rest'))\n",
    "    scan_stress='../stress_rest_reserve_100/stress{0}.mat'.format(i)\n",
    "    X_stress.append(readScan(scan_stress,'stress'))\n",
    "    scan_reserve='../stress_rest_reserve_100/reserve{0}.mat'.format(i)\n",
    "    X_reserve.append(readScan(scan_reserve,'reserve'))\n",
    "    # the first 50 description files have a different format \n",
    "    if i==51:\n",
    "        skip_rows=0\n",
    "    labels=pd.read_csv('../Data100/PD_ANN0%03d.csv' % (i), skiprows=skip_rows)\n",
    "    # Criteria to determine whether a polar map does not have risk = 0 or has risk = 1\n",
    "    LAD=labels['Reserve mean'][0]\n",
    "    LCX=labels['Reserve mean'][1]\n",
    "    RCA=labels['Reserve mean'][2]\n",
    "    y_temp=[0 if LAD > 2 and LCX > 2 and RCA > 2 else 1]\n",
    "    y.append(y_temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# Shuffle (permute) the scans\n",
    "rand_samples=random.sample(range(0,n_echoes),n_echoes)\n",
    "# we keep ids of the patients\n",
    "ids100=np.arange(100)+1\n",
    "ids=[ids100[i] for i in rand_samples]\n",
    "X_reserve_shuffle=[X_reserve[i] for i in rand_samples]\n",
    "# Make a deque object and rotate x times n-folds times\n",
    "X_=deque(X_reserve_shuffle)\n",
    "y_=deque(y);ids_=deque(ids)\n",
    "X_.rotate(20);y_.rotate(20);ids_.rotate(20)\n",
    "X_reserve_0=list(X_)\n",
    "y_0=to_categorical(list(y_))\n",
    "ids0=list(ids_)\n",
    "X_.rotate(20);y_.rotate(20);ids_.rotate(20)\n",
    "X_reserve_1=list(X_)\n",
    "y_1=to_categorical(list(y_))\n",
    "ids1=list(ids_)\n",
    "X_.rotate(20);y_.rotate(20);ids_.rotate(20)\n",
    "X_reserve_2=list(X_)\n",
    "y_2=to_categorical(list(y_))\n",
    "ids2=list(ids_)\n",
    "X_.rotate(20);y_.rotate(20);ids_.rotate(20)\n",
    "X_reserve_3=list(X_)\n",
    "y_3=to_categorical(list(y_))\n",
    "ids3=list(ids_)\n",
    "X_.rotate(20);y_.rotate(20);ids_.rotate(20)\n",
    "X_reserve_4=list(X_)\n",
    "y_4=to_categorical(list(y_))\n",
    "ids4=list(ids_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(X_reserve_shuffle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(X_reserve_0[20]),np.sum(X_reserve_1[40]),\n",
    "      np.sum(X_reserve_2[60]),np.sum(X_reserve_3[80]),\n",
    "      np.sum(X_reserve_4[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids0[0:5])\n",
    "print(ids1[20:25])\n",
    "print(ids2[40:45])\n",
    "print(ids3[60:65])\n",
    "print(ids4[80:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # permute the samples 5 times\n",
    "# X_reserve_5=[]\n",
    "# y_5=[]\n",
    "# for i in np.arange(5):\n",
    "#     rand_samples=random.sample(range(0,n_echoes),n_echoes)\n",
    "#     X_reserve_=[X_reserve[i] for i in rand_samples]\n",
    "#     y_=[y[i] for i in rand_samples]\n",
    "#     X_reserve_5.append(np.array(X_reserve_))\n",
    "#     y_5.append(to_categorical(np.array(y_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add padding to match inception shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## size for VGG = 224\n",
    "## size for inception = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_base_model_size(X,network='VGG'):\n",
    "    \"\"\"\n",
    "    Function does not generalize, made only for specific dimensions size\n",
    "    \"\"\"\n",
    "    pixels_base_model=[224 if network=='VGG' else 299]\n",
    "    ## For VGG\n",
    "    if int(pixels_base_model[0])-np.shape(X)[1]==1:\n",
    "        print('VGG')\n",
    "        pixels_to_add=1\n",
    "        padding=((0,pixels_to_add),(0,pixels_to_add),(0,0))\n",
    "        X=[np.pad(i,padding,mode='constant', constant_values=0) for i in X]\n",
    "    ## For Inception\n",
    "    elif int((pixels_base_model[0])-np.shape(X)[1])>=2:\n",
    "        pixels_to_add=int(((pixels_base_model[0])-np.shape(X)[1])/2)\n",
    "        print('Inception')\n",
    "        padding=((pixels_to_add,pixels_to_add),(pixels_to_add,pixels_to_add),(0,0))\n",
    "        X=[np.pad(i,padding,mode='constant', constant_values=0) for i in X]\n",
    "    else:\n",
    "        print('check dimensions') \n",
    "    return(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reserve_0=match_base_model_size(X_reserve_0)\n",
    "X_reserve_1=match_base_model_size(X_reserve_1)\n",
    "X_reserve_2=match_base_model_size(X_reserve_2)\n",
    "X_reserve_3=match_base_model_size(X_reserve_3)\n",
    "X_reserve_4=match_base_model_size(X_reserve_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_rest=match_base_model_size(X_rest,'VGG')\n",
    "# X_stress=match_base_model_size(X_stress,'VGG')\n",
    "# X_reserve=match_base_model_size(X_reserve,'VGG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_reserve_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(X_reserve_0[5][:,:,1])\n",
    "print(np.shape(X_reserve_0[5][:,:,1]))\n",
    "plt.figure()\n",
    "plt.pcolor(X_reserve_1[25][:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = applications.InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = applications.VGG19(weights = \"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmahttps://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5stery.com/save-load-keras-deep-learning-models/\n",
    "# serialize model to JSON\n",
    "# model_json = base_model.to_json()\n",
    "# with open(\"model_InceptionV3.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# base_model.save_weights(\"model_InceptionV3.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model_VGG19.json', 'r')\n",
    "#json_file = open('model_InceptionV3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "base_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "base_model.load_weights(\"model_VGG19.h5\")\n",
    "#base_model.load_weights(\"model_InceptionV3.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions   \n",
    "https://github.com/DeepLearningSandbox/DeepLearningSandbox/blob/master/transfer_learning/fine-tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://deeplearningsandbox.com/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "  \"\"\"Add last layer to the convnet\n",
    "  Args:\n",
    "    base_model: keras model excluding top\n",
    "    nb_classes: # of classes\n",
    "  Returns:\n",
    "    new keras model with last layer\n",
    "  \"\"\"\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(FC_SIZE, activation='relu')(x) \n",
    "  predictions = Dense(nb_classes, activation='softmax')(x) \n",
    "  model = Model(inputs=base_model.input, outputs=predictions)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_to_transfer_learn(model, base_model):\n",
    "  \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  model.compile(optimizer='Adam',    \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_to_finetune(model,learning_rate):\n",
    "   \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top \n",
    "      layers.\n",
    "   note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in \n",
    "         the inceptionv3 architecture\n",
    "   Args:\n",
    "     model: keras model\n",
    "   \"\"\"\n",
    "   for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "      layer.trainable = False\n",
    "   for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "      layer.trainable = True\n",
    "   model.compile(optimizer=SGD(lr=learning_rate, momentum=0.9),   \n",
    "                 loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_reserve_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes=2\n",
    "FC_SIZE = 1024\n",
    "NB_EPOCHS = 20\n",
    "BAT_SIZE = 32\n",
    "NB_IV3_LAYERS_TO_FREEZE = 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 7.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup model\n",
    "model = add_new_last_layer(base_model, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer learning\n",
    "setup_to_transfer_learn(model, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_tl=model.fit(X_reserve_0,y_0,batch_size=BAT_SIZE,\n",
    "                  epochs=NB_EPOCHS,shuffle=True,\n",
    "                    validation_split=.2,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_learn_XXX=pd.DataFrame(np.vstack((history_tl.history['acc'],\n",
    "                             history_tl.history['val_acc'],\n",
    "                             history_tl.history['loss'],\n",
    "                             history_tl.history['val_loss'])).T,\n",
    "                             columns=['acc','val_acc','loss','val_loss',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_learn_XXX['base_model']='Inception'\n",
    "trans_learn_XXX['input_image']='Xreserve_0'\n",
    "trans_learn_XXX['initial_lr']='0.0001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_learn_XXX.to_csv('Results/trans_learn_0XX_reserve_0.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_reserve_five[0][80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(y_5[0][80:],y_pred):\n",
    "    print(a[0],b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_=[i[0] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors=['#FF0000','#00FF00']\n",
    "a=[colors[int(i[0])] for i in y_5[0][80:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_=np.ones([1,20])+np.random.rand(1,20)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_,y_,s=50, c=a, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, bins, patches =plt.hist(np.asmatrix(y_pred)[:,0],bins=10,width=.005)\n",
    "jet = plt.get_cmap('jet', 2)\n",
    "for i in range(len(patches)):\n",
    "    patches[i].set_facecolor(jet(int(y_5[0][80:][i][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(patches)):\n",
    "    patches[i].set_facecolor(jet(int(y_5[0][80:][i][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_tl.history['acc'])\n",
    "plt.plot(history_tl.history['val_acc'],'g')\n",
    "plt.ylim([0.4,1])\n",
    "plt.title('ACC: VGG, Adam, lr=0.000001, step decay')\n",
    "plt.figure()\n",
    "plt.plot(history_tl.history['loss'])\n",
    "plt.plot(history_tl.history['val_loss'],'g')\n",
    "plt.title('Loss: VGG, Adam, lr=0.000001, step decay')\n",
    "plt.legend(['VGG, X_rest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_tl.history['acc'])\n",
    "plt.plot(history_tl.history['val_acc'],'g')\n",
    "plt.ylim([0.4,1])\n",
    "plt.title('ACC: VGG, Adam, lr=0.00001, step decay')\n",
    "plt.figure()\n",
    "plt.plot(history_tl.history['loss'])\n",
    "plt.plot(history_tl.history['val_loss'],'g')\n",
    "plt.title('Loss: VGG, Adam, lr=0.00001, step decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_tl.history['loss'])\n",
    "plt.plot(history_tl.history['val_loss'],'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=k.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # fine-tuning\n",
    "setup_to_finetune(model,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ft=model.fit(X,y,batch_size=BAT_SIZE,\n",
    "                  epochs=5,shuffle=True,\n",
    "                    validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training(history_tl,history_ft):\n",
    "  acc = history_tl.history['acc']+history_ft.history['acc']\n",
    "  val_acc = history_tl.history['val_acc']+history_ft.history['val_acc']\n",
    "  loss = history_tl.history['loss']+history_ft.history['loss']\n",
    "  val_loss = history_tl.history['val_loss']+history_ft.history['val_loss']\n",
    "  epochs = range(len(acc))\n",
    "  epochs_tl=len(history_tl.history['acc'])\n",
    "\n",
    "  plt.plot(epochs, acc, 'b')\n",
    "  plt.plot(epochs, val_acc, 'g')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.vlines(epochs_tl,0,1,linestyle='--')\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'b')\n",
    "  plt.plot(epochs, val_loss, 'g')\n",
    "  plt.title('Training and validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history_tl,history_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y[80:],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
